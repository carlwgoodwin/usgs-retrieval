{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bf69a1-9011-43c5-a380-33d33f224c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dataretrieval.nwis as nwis\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box, Polygon, MultiPolygon\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import earthaccess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298717eb-b1ff-4d33-9e18-043d2870f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    site_no                                         station_nm   dec_lat_va  \\\n",
      "0  09429500           COLORADO RIVER BELOW IMPERIAL DAM, AZ-CA  32.86758333   \n",
      "1  09522990  ALL-AMERICAN CANAL HEADWRKS AT IMPERIAL DAM, C...  32.88494444   \n",
      "2  10336610           UPPER TRUCKEE RV AT SOUTH LAKE TAHOE, CA   38.9224078   \n",
      "3  10336645                          GENERAL C NR MEEKS BAY CA  39.05185197   \n",
      "4  10336660                       BLACKWOOD C NR TAHOE CITY CA  39.10740708   \n",
      "\n",
      "    dec_long_va  \n",
      "0   -114.472111  \n",
      "1   -114.468111  \n",
      "2  -119.9915706  \n",
      "3  -120.1185209  \n",
      "4  -120.1621352  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set parameters for search\n",
    "param_codes = ['63680'] # turbidity\n",
    "param_codes_str = ','.join(param_codes) \n",
    "state_code = '06'\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "\n",
    "def get_param_sites(param_codes_str, state_code):\n",
    "\"\"\" retrieve sites with available data for parameter codes \"\"\"\n",
    "\n",
    "    url = f\"https://waterservices.usgs.gov/nwis/site/?format=rdb&stateCd={state_code}&parameterCd={param_codes_str}&startDT={start_date}&endDT={end_date}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        data_lines = [line for line in lines if not line.startswith('#') and line.strip()]\n",
    "        \n",
    "        if data_lines:\n",
    "            headers = data_lines[0].split('\\t')  # First line contains headers\n",
    "            data_lines = [line.split('\\t') for line in data_lines[2:]]  # Process the remaining data\n",
    "            df = pd.DataFrame(data_lines, columns=headers)\n",
    "            df_filter = df[['site_no', 'station_nm', 'dec_lat_va', 'dec_long_va']]\n",
    "            return df_filter\n",
    "    return pd.Dataframe()\n",
    "\n",
    "\n",
    "# function calls, prints\n",
    "site_data = get_param_sites(param_codes_str, state_code)\n",
    "#df_sites = pd.DataFrame(site_data)\n",
    "site_head_test = site_data.head()\n",
    "print(site_data.head())\n",
    "#site_data.to_csv('chla_sites_california.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d31b506-79bf-4ca0-a889-ae1d1a31cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-03T00:00:00Z,2023-09-03T00:00:00Z\n",
      "Retrieving granules for site: 09429500\n",
      "Retrieving granules for site: 09522990\n",
      "Retrieving granules for site: 10336610\n",
      "Retrieving granules for site: 10336645\n",
      "Retrieving granules for site: 10336660\n",
      "    site_no     site_lat     site_lon  \\\n",
      "0  09429500  32.86758333  -114.472111   \n",
      "1  09429500  32.86758333  -114.472111   \n",
      "2  09429500  32.86758333  -114.472111   \n",
      "3  09429500  32.86758333  -114.472111   \n",
      "4  09429500  32.86758333  -114.472111   \n",
      "\n",
      "                                        granule_urls  \\\n",
      "0  [https://data.lpdaac.earthdatacloud.nasa.gov/l...   \n",
      "1  [https://data.lpdaac.earthdatacloud.nasa.gov/l...   \n",
      "2  [https://data.lpdaac.earthdatacloud.nasa.gov/l...   \n",
      "3  [https://data.lpdaac.earthdatacloud.nasa.gov/l...   \n",
      "4  [https://data.lpdaac.earthdatacloud.nasa.gov/l...   \n",
      "\n",
      "                                        granule_bbox                  datetime  \n",
      "0  (-114.6581268, 32.2750206, -113.4960022, 33.42...  2022-08-13T23:26:05.000Z  \n",
      "1  (-115.229248, 31.7777786, -113.9893341, 32.999...  2023-06-06T17:59:30.000Z  \n",
      "2  (-114.6140289, 32.2974167, -113.3665466, 33.52...  2023-06-06T17:59:42.000Z  \n",
      "3  (-114.6408081, 32.225071, -113.4783325, 33.380...  2023-06-22T19:34:11.000Z  \n",
      "4  (-114.6136703, 32.1399879, -113.3629837, 33.37...  2023-07-25T22:29:53.000Z  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setup for granule search\n",
    "doi = '10.5067/EMIT/EMITL2ARFL.001'\n",
    "cmrurl = 'https://cmr.earthdata.nasa.gov/search/'\n",
    "doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "concept_id = requests.get(doisearch).json()['feed']['entry'][0]['id']\n",
    "start_date = dt.datetime(2021, 9, 3)\n",
    "end_date = dt.datetime(2023, 9, 3)  \n",
    "dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "temporal_str = start_date.strftime(dt_format) + ',' + end_date.strftime(dt_format)\n",
    "\n",
    "\n",
    "def get_site_granules(site_no, site_lat, site_lon):\n",
    "\"\"\" retrieve granules based on site coordinates \"\"\"\n",
    "\n",
    "    page_num = 1\n",
    "    page_size = 2000\n",
    "    granule_arr = []\n",
    "    \n",
    "    while True:\n",
    "        cmr_param = {\n",
    "            \"collection_concept_id\": concept_id,\n",
    "            \"page_size\": page_size,\n",
    "            \"page_num\": page_num,\n",
    "            \"temporal\": temporal_str,\n",
    "            \"point\": f\"{site_lon},{site_lat}\"  # Longitude first, then Latitude\n",
    "        }\n",
    "    \n",
    "        granulesearch = cmrurl + 'granules.json'\n",
    "        response = requests.post(granulesearch, data=cmr_param)\n",
    "        granules = response.json()['feed']['entry']\n",
    "    \n",
    "        if granules:\n",
    "            for g in granules:\n",
    "                granule_urls = [x['href'] for x in g['links'] if 'https' in x['href'] and '.nc' in x['href'] and '.dmrpp' not in x['href']]\n",
    "                granule_datetime = g.get('time_start', 'N/A')\n",
    "    \n",
    "                if 'polygons' in g:\n",
    "                    poly_coords = g['polygons'][0][0].split(\" \")\n",
    "                    poly_coords = [[float(poly_coords[i+1]), float(poly_coords[i])] for i in range(0, len(poly_coords), 2)]\n",
    "                    polygon = Polygon(poly_coords)\n",
    "                    granule_bbox = polygon.bounds  # Bounding box (min_lon, min_lat, max_lon, max_lat)\n",
    "                else:\n",
    "                    granule_bbox = 'N/A'\n",
    "                \n",
    "                granule_arr.append({\n",
    "                    \"site_no\": site_no,\n",
    "                    \"site_lat\": site_lat,\n",
    "                    \"site_lon\": site_lon,\n",
    "                    \"granule_urls\": granule_urls,\n",
    "                    \"granule_bbox\": granule_bbox,\n",
    "                    \"datetime\": granule_datetime\n",
    "                })\n",
    "                \n",
    "            page_num += 1\n",
    "        else:\n",
    "            break\n",
    "    return granule_arr\n",
    "\n",
    "\n",
    "def get_all_site_granules(site_data):\n",
    "\"\"\" combine data across sites \"\"\"\n",
    "\n",
    "    all_granules = []\n",
    "    for i, row in site_data.iterrows():\n",
    "        site_no = row['site_no']\n",
    "        site_lat = row['dec_lat_va']\n",
    "        site_lon = row['dec_long_va']\n",
    "        \n",
    "        print(f\"Retrieving granules for site: {site_no}\")    \n",
    "        site_granules = get_site_granules(site_no, site_lat, site_lon)\n",
    "        all_granules.extend(site_granules)\n",
    "    \n",
    "    return all_granules\n",
    "\n",
    "\n",
    "# function calls, prints\n",
    "all_site_granules = get_all_site_granules(site_head_test)\n",
    "df_granules = pd.DataFrame(all_site_granules)\n",
    "granule_head_test = df_granules.head()\n",
    "#print(all_site_granules)\n",
    "print(df_granules.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b4af07-a919-4fc9-a90b-a0cb48be827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_granule_results(site_no, param_cd, granule_time, time_window=5440):\n",
    "\"\"\" retrieve results for site and granule time \"\"\"\n",
    "    results = []\n",
    "\n",
    "    start_time = (granule_time - pd.Timedelta(minutes=time_window)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_time = (granule_time + pd.Timedelta(minutes=time_window)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    url = f\"https://waterservices.usgs.gov/nwis/iv/?format=json&sites={site_no}&parameterCd={param_cd}&startDT={start_time}&endDT={end_time}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'value' in data and 'timeSeries' in data['value']:\n",
    "            for ts in data['value']['timeSeries']:\n",
    "                for value in ts['values'][0]['value']:\n",
    "                    results.append({\n",
    "                        'chla_measurement': value['value'],\n",
    "                        'chla_unit': ts['variable']['unit']['unitCode'],\n",
    "                        'chla_time': value['dateTime']\n",
    "                    })\n",
    "    else:\n",
    "        print(f\"Failed to fetch Chla data for site {site_no}: {response.status_code}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# match each granule to chla results\n",
    "def match_granules_chla(df_granules, param_codes):\n",
    "\"\"\" match results with granules dataframe \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for index, row in df_granules.iterrows():\n",
    "        site_no = row['site_no']\n",
    "        granule_time = pd.to_datetime(row['datetime']) \n",
    "\n",
    "        for param_cd in param_codes:\n",
    "            chla_data = get_granule_results(site_no, param_cd, granule_time)\n",
    "            if chla_data:\n",
    "                for chla in chla_data:\n",
    "                    results.append({\n",
    "                        'site_no': site_no,\n",
    "                        'granule_time': granule_time,\n",
    "                        'granule_bbox': row['granule_bbox'],\n",
    "                        'granule_urls': row['granule_urls'],\n",
    "                        'chla_measurement': chla['chla_measurement'],\n",
    "                        'chla_unit': chla['chla_unit'],\n",
    "                        'chla_time': chla['chla_time']\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# function calls, prints\n",
    "df_granules_chla = match_granules_chla(granule_head_test, param_codes)\n",
    "print(df_granules_chla.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2085b55-d305-45e1-91c0-cd2b1bbfb464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
